The proposed model architecture addresses two key challenges in computational biology: RNA-conditional protein generation and protein binding prediction. It employs a Siamese network consisting of projection layers, transformer encoders, and attention pooling mechanisms to process both RNA and protein inputs into a shared 256-dimensional latent space. This Siamese structure enables the comparison of RNA and protein representations for binding prediction, while also serving as the basis for conditional protein generation. The decoder, built on transformer architecture, generates protein sequences based on the RNA latent representation, either using a custom projection or leveraging the ESM-2 language model head for enhanced biological relevance.
