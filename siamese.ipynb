{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["mwSAUudNNQ-y","PW7QVocj0p4a","HHfD9N6nN7xS","Wwc0AIjD1aq1","FVScx-_8Lyfd"],"gpuType":"V28","mount_file_id":"1nYifoINqno2zb6SEEeOByZCoYUW_0OKZ","authorship_tag":"ABX9TyPmj4EYNH1zFij2FE/RCaxd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# Project MOML - RBP Siamese\n","\n","RNA-RBP Interaction Prediction and RBP Generation Process:\n","\n","Input (RNA + RBP sequences)\n","\n","  ↓\n","\n","a) Embedding Layer (Fusion of rnabert+ESM)\n","\n","  ↓\n","\n","b) T5 Encoder Stacks (RNA and RBP)\n","\n","  ↓\n","\n","c) Latent Space Projection\n","  \n","  ↓\n","\n","d) Cross-Attention Layer\n","\n","  ↓\n","\n","e) Fusion Layer\n","\n","  ↓\n","\n","f) T5 Decoder Stack\n","\n","  ↓\n","\n","g) Language Modeling Head (ESM)\n","\n","  ↓\n","\n","Output (Interaction Prediction + Generated RBP sequence)\n","\n","Detailed Description:\n","\n","The process begins with input RNA and RBP sequences, which are tokenized and converted to integer indices. These indices are passed through a shared embedding layer (a) that transforms them into dense vector representations. The embedded sequences then enter separate but identical T5 Encoder Stacks (b), each consisting of multiple layers of self-attention mechanisms and feed-forward networks. These encoders capture the contextual information within each sequence. The encoder outputs are projected into a shared latent space (c), where a cross-attention layer (d) allows for interaction between the RNA and RBP representations. This interaction is crucial for capturing the relationship between RNA motifs and their binding proteins. The cross-attended representations are then combined in a fusion layer (e), producing a single representation that encapsulates the joint RNA-RBP information. This fused representation serves as input to the T5 Decoder Stack (f), which generates the RBP sequence through a series of self-attention, cross-attention, and feed-forward layers. Finally, a language modeling head (g) projects the decoder output to the vocabulary space, producing probabilities for each amino acid at each position. The model outputs both an interaction prediction (derived from the similarity of the latent representations) and a generated RBP sequence. Throughout this process, the model learns to encode meaningful representations of RNA and RBP sequences, predict their interactions, and generate novel RBP sequences tailored to specific RNA inputs."],"metadata":{"id":"E1yudl-wNUIV"}},{"cell_type":"markdown","source":["##gdrive"],"metadata":{"id":"mwSAUudNNQ-y"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"H9MVHS_jxNyk","executionInfo":{"status":"ok","timestamp":1726188989256,"user_tz":420,"elapsed":188,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/rna/aidrugsx-siamese network')"]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgZAqNdUyjaT","executionInfo":{"status":"ok","timestamp":1726188990306,"user_tz":420,"elapsed":505,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}},"outputId":"3d3b7dc3-7618-4674-f753-c13e41bd078c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["final_attract_db_with_emb.csv  rbp_seqs_dict.pkl  rna_motif_emb.npy  siamese.ipynb\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GF0lkDCtNT2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## generate dataset"],"metadata":{"id":"PW7QVocj0p4a"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"yZ176Aj0etVV","executionInfo":{"status":"ok","timestamp":1726188995129,"user_tz":420,"elapsed":163,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Z7Hl_M7zeVnA","executionInfo":{"status":"ok","timestamp":1726188997020,"user_tz":420,"elapsed":1717,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["data = pd.read_csv('final_attract_db_with_emb.csv')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"T6AMmpLfAr5x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726189010505,"user_tz":420,"elapsed":13488,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}},"outputId":"52ddc8e5-26f2-438c-c885-317de00b7b19"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]}],"source":["import pickle\n","# Load the dictionary back from the pickle file.\n","with open(\"rbp_seqs_dict.pkl\", \"rb\") as f:\n","    rbp_seqs_dict = pickle.load(f)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NOBIK9NqBmux","executionInfo":{"status":"ok","timestamp":1726189012596,"user_tz":420,"elapsed":2105,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["import numpy as np\n","rna_motif_emb = np.load('rna_motif_emb.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726189012596,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"},"user_tz":420},"id":"gR5oupoyDg0_","outputId":"7b30916a-5796-4995-99a7-87ae823eb743"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":9}],"source":["len(rna_motif_emb[1])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"XlC-0ezvDnxN","executionInfo":{"status":"ok","timestamp":1726189012596,"user_tz":420,"elapsed":5,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["data = data.drop(columns=['rna_motif_emb', 'rbp_esm_emb'])"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"476-y3RlEQGe","executionInfo":{"status":"ok","timestamp":1726189012597,"user_tz":420,"elapsed":5,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["data['rna_motif_emb'] = rna_motif_emb"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ek95MlFMEoTp","executionInfo":{"status":"ok","timestamp":1726189017297,"user_tz":420,"elapsed":4705,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["data['rbp_esm_emb'] = data['RBP_sequence'].map(rbp_seqs_dict)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"r45nc1dZEuPy","executionInfo":{"status":"ok","timestamp":1726189025958,"user_tz":420,"elapsed":8664,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"outputs":[],"source":["# Convert list of tensors to numpy array\n","def tensors_to_numpy(tensor_list):\n","    return np.stack([t.numpy() for t in tensor_list])\n","\n","# Apply the conversion to the 'rbp_esm_emb' column\n","data['rbp_esm_emb'] = data['rbp_esm_emb'].apply(tensors_to_numpy)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1726189026364,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"},"user_tz":420},"id":"xY37cMdeFAlM","outputId":"c72e8e87-f3ff-4879-f1aa-c997e55849c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  Gene_name          Gene_id        Motif  \\\n","0      A1CF  ENSG00000148584  UGAUCAGUAUA   \n","1      A1CF  ENSG00000148584      AUAAUUA   \n","2      A1CF  ENSG00000148584      UUAAUUA   \n","3      A1CF  ENSG00000148584      AUAAUUG   \n","4      A1CF  ENSG00000148584      UUAAUUG   \n","\n","                                        RBP_sequence  \\\n","0  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n","1  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n","2  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n","3  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n","4  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n","\n","                                       rna_motif_emb  \\\n","0  [[0.02515462040901184, -0.1693081259727478, 1....   \n","1  [[-0.005481339991092682, -0.008752591907978058...   \n","2  [[-0.006157027557492256, -0.04564734920859337,...   \n","3  [[0.0016195997595787048, 0.0697508156299591, 1...   \n","4  [[0.02757852151989937, 0.006868686527013779, 3...   \n","\n","                                         rbp_esm_emb  \n","0  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n","1  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n","2  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n","3  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n","4  [[0.102689214, -0.18220823, -0.05008613, 0.156...  "],"text/html":["\n","  <div id=\"df-d9ca7f4a-d882-467e-83f5-263dd632fdfe\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Gene_name</th>\n","      <th>Gene_id</th>\n","      <th>Motif</th>\n","      <th>RBP_sequence</th>\n","      <th>rna_motif_emb</th>\n","      <th>rbp_esm_emb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A1CF</td>\n","      <td>ENSG00000148584</td>\n","      <td>UGAUCAGUAUA</td>\n","      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n","      <td>[[0.02515462040901184, -0.1693081259727478, 1....</td>\n","      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A1CF</td>\n","      <td>ENSG00000148584</td>\n","      <td>AUAAUUA</td>\n","      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n","      <td>[[-0.005481339991092682, -0.008752591907978058...</td>\n","      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A1CF</td>\n","      <td>ENSG00000148584</td>\n","      <td>UUAAUUA</td>\n","      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n","      <td>[[-0.006157027557492256, -0.04564734920859337,...</td>\n","      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A1CF</td>\n","      <td>ENSG00000148584</td>\n","      <td>AUAAUUG</td>\n","      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n","      <td>[[0.0016195997595787048, 0.0697508156299591, 1...</td>\n","      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A1CF</td>\n","      <td>ENSG00000148584</td>\n","      <td>UUAAUUG</td>\n","      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n","      <td>[[0.02757852151989937, 0.006868686527013779, 3...</td>\n","      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9ca7f4a-d882-467e-83f5-263dd632fdfe')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d9ca7f4a-d882-467e-83f5-263dd632fdfe button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d9ca7f4a-d882-467e-83f5-263dd632fdfe');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e59eb906-a4e6-406b-b371-ddab3c3d313a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e59eb906-a4e6-406b-b371-ddab3c3d313a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e59eb906-a4e6-406b-b371-ddab3c3d313a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 3252,\n  \"fields\": [\n    {\n      \"column\": \"Gene_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 158,\n        \"samples\": [\n          \"SRSF10\",\n          \"HNRNPAB\",\n          \"SRSF6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gene_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 159,\n        \"samples\": [\n          \"ENSG00000162231\",\n          \"ENSG00000152518\",\n          \"ENSG00000136450\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Motif\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2294,\n        \"samples\": [\n          \"ACCACGCA\",\n          \"AGGAGC\",\n          \"UGGGGAU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RBP_sequence\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 158,\n        \"samples\": [\n          \"MSRYLRPPNTSLFVRNVADDTRSEDLRREFGRYGPIVDVYVPLDFYTRRPRGFAYVQFEDVRDAEDALHNLDRKWICGRQIEIQFAQGDRKTPNQMKAKEGRNVYSSSRYDDYDRYRRSRSRSYERRRSRSRSFDYNYRRSYSPRNSRPTGRPRRSRSHSDNDRFKHRNRSFSRSKSNSRSRSKSQPKKEMKAKSRSRSASHTKTRGTSKTDSKTHYKSGSRYEKESRKKEPPRSKSQSRSQSRSRSKSRSRSWTSPKSSGH\",\n          \"MSEAGEEQPMETTGATENGHEAVPEASRGRGWTGAAAGAGGATAAPPSGNQNGAEGDQINASKNEEDAGKMFVGGLSWDTSKKDLKDYFTKFGEVVDCTIKMDPNTGRSRGFGFILFKDAASVEKVLDQKEHRLDGRVIDPKKAMAMKKDPVKKIFVGGLNPESPTEEKIREYFGEFGEIEAIELPMDPKLNKRRGFVFITFKEEEPVKKVLEKKFHTVSGSKCEIKVAQPKEVYQQQQYGSGGRGNRNRGNRGSGGGGGGGGQSQSWNQGYGNYWNQGYGYQQGYGPGYGGYDYSPYGYYGYGPGYDYSQGSTNYGKSQRRGGHQNNYKPY\",\n          \"MPRVYIGRLSYNVREKDIQRFFSGYGRLLEVDLKNGYGFVEFEDSRDADDAVYELNGKELCGERVIVEHARGPRRDRDGYSYGSRSGGGGYSSRRTSGRDKYGPPVRTEYRLIVENLSSRCSWQDLKDFMRQAGEVTYADAHKERTNEGVIEFRSYSDMKRALDKLDGTEINGRNIRLIEDKPRTSHRRSYSGSRSRSRSRRRSRSRSRRSSRSRSRSISKSRSRSRSRSKGRSRSRSKGRKSRSKSKSKPKSDRGSHSHSRSRSKDEYEKSRSRSRSRSPKENGKGDIKSKSRSRSQSRSNSPLPVPPSKARSVSPPPKRATSRSRSRSRSKSRSRSRSSSRD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rna_motif_emb\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rbp_esm_emb\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}],"source":["data.head()"]},{"cell_type":"markdown","source":["## fusion layer"],"metadata":{"id":"HHfD9N6nN7xS"}},{"cell_type":"code","source":["# fusion"],"metadata":{"id":"HtCNc3Nj1Jm5","executionInfo":{"status":"ok","timestamp":1726189026365,"user_tz":420,"elapsed":4,"user":{"displayName":"Venkata Srikar Kavirayuni","userId":"10910189441313353201"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class EnhancedEmbeddingFusionModule(nn.Module):\n","    def __init__(self, rna_dim=120, protein_dim=1280, fusion_dim=512, num_heads=8):\n","        super(EnhancedEmbeddingFusionModule, self).__init__()\n","\n","        self.rna_projection = nn.Linear(rna_dim, fusion_dim)\n","        self.protein_projection = nn.Linear(protein_dim, fusion_dim)\n","\n","        self.cross_attention = MultiHeadAttention(fusion_dim, num_heads)\n","\n","        self.fusion_layer = nn.Sequential(\n","            nn.Linear(fusion_dim * 2, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.ReLU(),\n","            nn.Linear(fusion_dim, fusion_dim)\n","        )\n","\n","        self.enhanced_gating_mechanism = EnhancedGatingMechanism(fusion_dim)\n","\n","    def forward(self, rna_emb, protein_emb):\n","        rna_proj = self.rna_projection(rna_emb)\n","        protein_proj = self.protein_projection(protein_emb)\n","\n","        rna_attended = self.cross_attention(rna_proj, protein_proj, protein_proj)\n","        protein_attended = self.cross_attention(protein_proj, rna_proj, rna_proj)\n","\n","        concat_features = torch.cat([rna_attended, protein_attended], dim=-1)\n","        fused_features = self.fusion_layer(concat_features)\n","\n","        gated_output = self.enhanced_gating_mechanism(rna_attended, protein_attended, fused_features)\n","\n","        return gated_output\n","\n","class EnhancedGatingMechanism(nn.Module):\n","    def __init__(self, fusion_dim):\n","        super(EnhancedGatingMechanism, self).__init__()\n","\n","        self.rna_gate = nn.Sequential(\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.ReLU(),\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.Sigmoid()\n","        )\n","\n","        self.protein_gate = nn.Sequential(\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.ReLU(),\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.Sigmoid()\n","        )\n","\n","        self.fusion_gate = nn.Sequential(\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.ReLU(),\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.Sigmoid()\n","        )\n","\n","        self.final_gate = nn.Sequential(\n","            nn.Linear(fusion_dim * 3, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.ReLU(),\n","            nn.Linear(fusion_dim, 3),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","        self.output_projection = nn.Sequential(\n","            nn.Linear(fusion_dim, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.ReLU(),\n","            nn.Linear(fusion_dim, fusion_dim)\n","        )\n","\n","    def forward(self, rna_features, protein_features, fused_features):\n","        rna_gate = self.rna_gate(rna_features)\n","        protein_gate = self.protein_gate(protein_features)\n","        fusion_gate = self.fusion_gate(fused_features)\n","\n","        gated_rna = rna_gate * rna_features\n","        gated_protein = protein_gate * protein_features\n","        gated_fusion = fusion_gate * fused_features\n","\n","        combined_features = torch.cat([gated_rna, gated_protein, gated_fusion], dim=-1)\n","        final_gate = self.final_gate(combined_features)\n","\n","        gated_output = (final_gate[:,:,0].unsqueeze(-1) * gated_rna +\n","                        final_gate[:,:,1].unsqueeze(-1) * gated_protein +\n","                        final_gate[:,:,2].unsqueeze(-1) * gated_fusion)\n","\n","        return self.output_projection(gated_output)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","        attn_probs = F.softmax(attn_scores, dim=-1)\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    def forward(self, Q, K, V, mask=None):\n","        batch_size = Q.size(0)\n","\n","        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n","        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n","        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n","\n","        output = self.scaled_dot_product_attention(Q, K, V, mask)\n","        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n","        return self.W_o(output)\n"],"metadata":{"id":"1lJmVShDOeGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## siamese t5\n","\n","Architecture Brief:\n","The Siamese T5 Generator is an innovative architecture designed for RNA-protein binding prediction and generation. It combines the power of the T5 (Text-to-Text Transfer Transformer) model with a Siamese network structure, enabling both sequence comparison and generation tasks.\n","The core of the architecture consists of two identical T5-based encoder stacks that process RNA and protein sequences independently. These encoders utilize self-attention mechanisms and feed-forward networks, incorporating relative position embeddings for enhanced spatial awareness. The encoded representations are projected into a shared latent space, where a cross-attention mechanism facilitates information exchange between RNA and protein features. This interaction is crucial for capturing the nuanced relationships between RNA motifs and their binding proteins.\n","The decoder stack, also based on the T5 architecture, takes the fused representation of RNA and protein information as input. It employs a combination of self-attention and cross-attention layers to generate novel protein sequences conditioned on the input RNA motif. The use of gated feed-forward layers and layer normalization throughout the network enhances its expressive power and training stability. The final output is produced through a language modeling head, enabling the model to generate amino acid sequences. This architecture not only allows for binding prediction but also for the de novo design of proteins tailored to specific RNA motifs."],"metadata":{"id":"Wwc0AIjD1aq1"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SiameseT5Generator(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","        self.encoder_stack = T5Stack(config, is_decoder=False)\n","        self.decoder_stack = T5Stack(config, is_decoder=True)\n","        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n","        self.latent_projector = nn.Linear(config.d_model, config.latent_dim)\n","        self.fusion_layer = nn.Linear(config.latent_dim * 2, config.latent_dim)\n","\n","    def forward(self, rna_ids, rbp_ids, decoder_input_ids=None):\n","        rna_emb = self.shared(rna_ids)\n","        rbp_emb = self.shared(rbp_ids)\n","\n","        rna_enc = self.encoder_stack(rna_emb)\n","        rbp_enc = self.encoder_stack(rbp_emb)\n","\n","        rna_latent = self.latent_projector(rna_enc[0])\n","        rbp_latent = self.latent_projector(rbp_enc[0])\n","\n","        fused = self.fusion_layer(torch.cat([rna_latent, rbp_latent], dim=-1))\n","\n","        if decoder_input_ids is None:\n","            decoder_input_ids = self._shift_right(rbp_ids)\n","\n","        decoder_outputs = self.decoder_stack(\n","            input_ids=decoder_input_ids,\n","            encoder_hidden_states=fused,\n","            encoder_attention_mask=None\n","        )\n","\n","        lm_logits = self.lm_head(decoder_outputs[0])\n","\n","        return rna_latent, rbp_latent, lm_logits\n","\n","    def _shift_right(self, input_ids):\n","        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n","        shifted_input_ids[..., 0] = self.config.decoder_start_token_id\n","        return shifted_input_ids\n","\n","class T5Stack(nn.Module):\n","    def __init__(self, config, is_decoder=False):\n","        super().__init__()\n","        self.is_decoder = is_decoder\n","        self.block = nn.ModuleList([T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)])\n","        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, input_ids=None, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None):\n","        hidden_states = self.dropout(self.shared(input_ids))\n","\n","        for i, layer_module in enumerate(self.block):\n","            layer_outputs = layer_module(\n","                hidden_states,\n","                attention_mask=attention_mask,\n","                encoder_hidden_states=encoder_hidden_states,\n","                encoder_attention_mask=encoder_attention_mask,\n","            )\n","            hidden_states = layer_outputs[0]\n","\n","        hidden_states = self.final_layer_norm(hidden_states)\n","        return (hidden_states,)\n","\n","class T5Block(nn.Module):\n","    def __init__(self, config, has_relative_attention_bias=False):\n","        super().__init__()\n","        self.is_decoder = config.is_decoder\n","        self.layer = nn.ModuleList()\n","        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n","        if self.is_decoder:\n","            self.layer.append(T5LayerCrossAttention(config))\n","        self.layer.append(T5LayerFF(config))\n","\n","    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None):\n","        self_attention_outputs = self.layer[0](hidden_states, attention_mask=attention_mask)\n","        hidden_states = self_attention_outputs[0]\n","\n","        if self.is_decoder and encoder_hidden_states is not None:\n","            cross_attention_outputs = self.layer[1](hidden_states, encoder_hidden_states, encoder_attention_mask)\n","            hidden_states = cross_attention_outputs[0]\n","\n","        feed_forward_outputs = self.layer[-1](hidden_states)\n","        hidden_states = feed_forward_outputs[0]\n","\n","        return (hidden_states,)\n","\n","class T5LayerSelfAttention(nn.Module):\n","    def __init__(self, config, has_relative_attention_bias=False):\n","        super().__init__()\n","        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n","        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, hidden_states, attention_mask=None):\n","        normed_hidden_states = self.layer_norm(hidden_states)\n","        attention_output = self.SelfAttention(normed_hidden_states, mask=attention_mask)\n","        hidden_states = hidden_states + self.dropout(attention_output[0])\n","        return (hidden_states,)\n","\n","class T5LayerCrossAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n","        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, hidden_states, encoder_hidden_states, encoder_attention_mask):\n","        normed_hidden_states = self.layer_norm(hidden_states)\n","        attention_output = self.EncDecAttention(\n","            normed_hidden_states,\n","            mask=encoder_attention_mask,\n","            key_value_states=encoder_hidden_states\n","        )\n","        layer_output = hidden_states + self.dropout(attention_output[0])\n","        return (layer_output,)\n","\n","class T5LayerFF(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.DenseReluDense = T5DenseGatedActDense(config)\n","        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, hidden_states):\n","        forwarded_states = self.layer_norm(hidden_states)\n","        forwarded_states = self.DenseReluDense(forwarded_states)\n","        hidden_states = hidden_states + self.dropout(forwarded_states)\n","        return (hidden_states,)\n","\n","class T5DenseGatedActDense(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n","        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n","        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","        self.act = F.gelu\n","\n","    def forward(self, hidden_states):\n","        hidden_gelu = self.act(self.wi_0(hidden_states))\n","        hidden_linear = self.wi_1(hidden_states)\n","        hidden_states = hidden_gelu * hidden_linear\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.wo(hidden_states)\n","        return hidden_states\n","\n","class T5LayerNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-6):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, hidden_states):\n","        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n","        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n","        return self.weight * hidden_states\n","\n","class T5Attention(nn.Module):\n","    def __init__(self, config, has_relative_attention_bias=False):\n","        super().__init__()\n","        self.is_decoder = config.is_decoder\n","        self.has_relative_attention_bias = has_relative_attention_bias\n","        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n","        self.d_model = config.d_model\n","        self.key_value_proj_dim = config.d_kv\n","        self.n_heads = config.num_heads\n","        self.dropout = config.dropout_rate\n","        self.inner_dim = self.n_heads * self.key_value_proj_dim\n","\n","        self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n","        self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n","        self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)\n","        self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)\n","\n","        if self.has_relative_attention_bias:\n","            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n","\n","    def forward(self, hidden_states, mask=None, key_value_states=None, position_bias=None):\n","        batch_size, seq_length = hidden_states.shape[:2]\n","\n","        def shape(states):\n","            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n","\n","        def unshape(states):\n","            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n","\n","        query_states = shape(self.q(hidden_states))\n","\n","        if key_value_states is None:\n","            key_states = shape(self.k(hidden_states))\n","            value_states = shape(self.v(hidden_states))\n","        else:\n","            key_states = shape(self.k(key_value_states))\n","            value_states = shape(self.v(key_value_states))\n","\n","        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n","\n","        if position_bias is None:\n","            if not self.has_relative_attention_bias:\n","                position_bias = torch.zeros(\n","                    (1, self.n_heads, seq_length, seq_length),\n","                    device=scores.device,\n","                    dtype=scores.dtype\n","                )\n","            else:\n","                position_bias = self.compute_bias(seq_length)\n","\n","            if mask is not None:\n","                position_bias = position_bias + mask\n","\n","        scores += position_bias\n","        attn_weights = F.softmax(scores.float(), dim=-1).type_as(scores)\n","        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n","\n","        attn_output = unshape(torch.matmul(attn_weights, value_states))\n","        attn_output = self.o(attn_output)\n","\n","        return (attn_output, position_bias)\n","\n","    def compute_bias(self, seq_length):\n","        context_position = torch.arange(seq_length, dtype=torch.long)[:, None]\n","        memory_position = torch.arange(seq_length, dtype=torch.long)[None, :]\n","        relative_position = memory_position - context_position\n","        relative_position_bucket = self._relative_position_bucket(\n","            relative_position,\n","            bidirectional=(not self.is_decoder),\n","            num_buckets=self.relative_attention_num_buckets\n","        )\n","        values = self.relative_attention_bias(relative_position_bucket)\n","        values = values.permute([2, 0, 1]).unsqueeze(0)\n","        return values\n","\n","    @staticmethod\n","    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n","        relative_buckets = 0\n","        if bidirectional:\n","            num_buckets //= 2\n","            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n","            relative_position = torch.abs(relative_position)\n","        else:\n","            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n","        max_exact = num_buckets // 2\n","        is_small = relative_position < max_exact\n","        relative_postion_if_large = max_exact + (\n","            torch.log(relative_position.float() / max_exact)\n","            / math.log(max_distance / max_exact)\n","            * (num_buckets - max_exact)\n","        ).to(torch.long)\n","        relative_postion_if_large = torch.min(\n","            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n","        )\n","        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)\n","        return relative_buckets"],"metadata":{"id":"vrdOyzvk7yDr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## training (dummy dataset)\n","\n","dummy dataset ex.\n","\n","RNA sequence: \"AUGGCUAUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCU\"\n","RBP sequence: \"MKVILWAALVITFLAGCQAETEPEPELRQQTEWQSGQRWELALGRFWDYLRWVQTLSEQVQEELLSSQVTQELRALM\"\n","Label: 1 (positive interaction)\n","\n","RNA sequence: \"CGAUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCU\"\n","RBP sequence: \"MKVLWAALLVTFLAGCQAKVEQAVETEPEPELRQQTEWQSGQRWELALGRFWDYLRWVQTLSEQVQEELLSSQVTQ\"\n","Label: 0 (negative interaction)\n","\n","RNA sequence: \"UAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGCUAGC\"\n","RBP sequence: \"MELKAYKSELEEQLTPVAEETRARLSKELQAAQARLGADVLASHGRLVQYRGEVQAMLGQSTEELRVRLASHLRKL\"\n","Label: 1 (positive interaction)\n","\n","In these examples:\n","\n","RNA sequences are strings of A, U, G, and C.\n","RBP sequences are strings of amino acid single-letter codes.\n","The label is either 0 (no interaction) or 1 (interaction).\n","Sequence lengths vary but are within the specified maximum length.\n","\n","In the actual implementation, these would be converted to tensor representations of integer indices corresponding to each nucleotide or amino acid (tokenization)."],"metadata":{"id":"FVScx-_8Lyfd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import numpy as np\n","\n","class Config:\n","    vocab_size = 30\n","    d_model = 512\n","    d_ff = 2048\n","    num_layers = 6\n","    num_heads = 8\n","    dropout_rate = 0.1\n","    layer_norm_epsilon = 1e-6\n","    max_seq_length = 100\n","    latent_dim = 256\n","    num_decoder_layers = 6\n","    decoder_start_token_id = 0\n","    batch_size = 32\n","    num_epochs = 10\n","    learning_rate = 3e-4\n","\n","config = Config()\n","\n","class DummyDataset(Dataset):\n","    def __init__(self, num_samples=1000, max_length=100):\n","        self.num_samples = num_samples\n","        self.max_length = max_length\n","        self.data = self.generate_dummy_data()\n","\n","    def generate_dummy_data(self):\n","        data = []\n","        for _ in range(self.num_samples):\n","            rna_length = random.randint(20, self.max_length)\n","            rbp_length = random.randint(20, self.max_length)\n","            rna = torch.randint(1, config.vocab_size, (rna_length,))\n","            rbp = torch.randint(1, config.vocab_size, (rbp_length,))\n","            label = random.randint(0, 1)\n","            data.append((rna, rbp, label))\n","        return data\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","def collate_fn(batch):\n","    rna_seqs, rbp_seqs, labels = zip(*batch)\n","    rna_lengths = [len(seq) for seq in rna_seqs]\n","    rbp_lengths = [len(seq) for seq in rbp_seqs]\n","    max_rna_len = max(rna_lengths)\n","    max_rbp_len = max(rbp_lengths)\n","\n","    padded_rna = torch.zeros(len(batch), max_rna_len, dtype=torch.long)\n","    padded_rbp = torch.zeros(len(batch), max_rbp_len, dtype=torch.long)\n","\n","    for i, (rna, rbp) in enumerate(zip(rna_seqs, rbp_seqs)):\n","        padded_rna[i, :len(rna)] = rna\n","        padded_rbp[i, :len(rbp)] = rbp\n","\n","    return padded_rna, padded_rbp, torch.tensor(labels)\n","\n","train_dataset = DummyDataset()\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n","\n","model = SiameseT5Generator(config)\n","optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n","\n","contrastive_loss = nn.CosineEmbeddingLoss()\n","ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n","\n","def train_epoch(model, dataloader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","\n","    for rna_ids, rbp_ids, labels in dataloader:\n","        rna_ids, rbp_ids, labels = rna_ids.to(device), rbp_ids.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        rna_latent, rbp_latent, lm_logits = model(rna_ids, rbp_ids)\n","\n","        loss_contrastive = contrastive_loss(rna_latent, rbp_latent, (labels * 2 - 1).float())\n","\n","        loss_ce = ce_loss(lm_logits.view(-1, config.vocab_size), rbp_ids.view(-1))\n","\n","        loss = loss_contrastive + loss_ce\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(config.num_epochs):\n","    avg_loss = train_epoch(model, train_loader, optimizer, device)\n","    print(f\"Epoch {epoch+1}/{config.num_epochs}, Loss: {avg_loss:.4f}\")\n","\n","def generate_rbp(model, rna_sequence, max_length=100):\n","    model.eval()\n","    with torch.no_grad():\n","        rna_ids = rna_sequence.unsqueeze(0).to(device)\n","        decoder_input = torch.tensor([[config.decoder_start_token_id]]).to(device)\n","\n","        for _ in range(max_length):\n","            rna_latent, _, lm_logits = model(rna_ids, decoder_input)\n","            next_token = lm_logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n","            decoder_input = torch.cat([decoder_input, next_token], dim=-1)\n","\n","            if next_token.item() == config.decoder_start_token_id:\n","                break\n","\n","    return decoder_input.squeeze().cpu().numpy()\n","\n","test_rna = torch.randint(1, config.vocab_size, (50,)).to(device)\n","generated_rbp = generate_rbp(model, test_rna)\n","print(\"Generated RBP sequence:\", generated_rbp)\n","\n","def predict_binding(model, rna_sequence, rbp_sequence):\n","    model.eval()\n","    with torch.no_grad():\n","        rna_ids = rna_sequence.unsqueeze(0).to(device)\n","        rbp_ids = rbp_sequence.unsqueeze(0).to(device)\n","        rna_latent, rbp_latent, _ = model(rna_ids, rbp_ids)\n","        similarity = nn.functional.cosine_similarity(rna_latent, rbp_latent)\n","    return similarity.item()\n","\n","test_rna = torch.randint(1, config.vocab_size, (50,)).to(device)\n","test_rbp = torch.randint(1, config.vocab_size, (50,)).to(device)\n","binding_score = predict_binding(model, test_rna, test_rbp)\n","print(\"Binding prediction score:\", binding_score)"],"metadata":{"id":"Tk7Gar5qJyYt"},"execution_count":null,"outputs":[]}]}