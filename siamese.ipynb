{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mwSAUudNNQ-y",
        "PW7QVocj0p4a",
        "KgWejYjbHKsZ",
        "n--6yv9O_PD8"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project MOML - RBP Siamese\n",
        "\n"
      ],
      "metadata": {
        "id": "E1yudl-wNUIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gdrive"
      ],
      "metadata": {
        "id": "mwSAUudNNQ-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H9MVHS_jxNyk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/pbg-shortcuts/rna/aidrugsx-siamese network')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgZAqNdUyjaT",
        "outputId": "4559c4e7-1d75-4896-98d4-8a45862e963e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "condacolab_install.log\t       notes.gdoc\t  rna_motif_emb.npy\n",
            "final_attract_db_with_emb.csv  rbp_seqs_dict.pkl  siamese.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fair-esm"
      ],
      "metadata": {
        "id": "GF0lkDCtNT2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8985e1-b44c-4793-80db-f8d915ee43b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.10/dist-packages (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##setting up ESM\n",
        "import torch\n",
        "import esm\n",
        "\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model.eval()  # disables dropout for deterministic results\n",
        "esm_model.cuda() #push model to gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtNAxd1l6epI",
        "outputId": "02e73961-d0fd-42f0-d0b1-9fb1a71a3e1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0-32): 33 x TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate dataset"
      ],
      "metadata": {
        "id": "PW7QVocj0p4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yZ176Aj0etVV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z7Hl_M7zeVnA"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('final_attract_db_with_emb.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T6AMmpLfAr5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6107ed1b-a204-4011-9fb4-0fea2b21423a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(io.BytesIO(b))\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "# Load the dictionary back from the pickle file.\n",
        "with open(\"rbp_seqs_dict.pkl\", \"rb\") as f:\n",
        "    rbp_seqs_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NOBIK9NqBmux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rna_motif_emb = np.load('rna_motif_emb.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR5oupoyDg0_",
        "outputId": "2a2052dd-13d9-4335-c8f4-5a05c47741ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(rna_motif_emb[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XlC-0ezvDnxN"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['rna_motif_emb', 'rbp_esm_emb'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "476-y3RlEQGe"
      },
      "outputs": [],
      "source": [
        "data['rna_motif_emb'] = rna_motif_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ek95MlFMEoTp"
      },
      "outputs": [],
      "source": [
        "data['rbp_esm_emb'] = data['RBP_sequence'].map(rbp_seqs_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r45nc1dZEuPy"
      },
      "outputs": [],
      "source": [
        "# Convert list of tensors to numpy array\n",
        "def tensors_to_numpy(tensor_list):\n",
        "    return np.stack([t.numpy() for t in tensor_list])\n",
        "\n",
        "# Apply the conversion to the 'rbp_esm_emb' column\n",
        "data['rbp_esm_emb'] = data['rbp_esm_emb'].apply(tensors_to_numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xY37cMdeFAlM",
        "outputId": "5f7536c9-c6e5-4a0a-bd1e-fe86448fc06b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Gene_name          Gene_id        Motif  \\\n",
              "0      A1CF  ENSG00000148584  UGAUCAGUAUA   \n",
              "1      A1CF  ENSG00000148584      AUAAUUA   \n",
              "2      A1CF  ENSG00000148584      UUAAUUA   \n",
              "3      A1CF  ENSG00000148584      AUAAUUG   \n",
              "4      A1CF  ENSG00000148584      UUAAUUG   \n",
              "\n",
              "                                        RBP_sequence  \\\n",
              "0  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n",
              "1  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n",
              "2  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n",
              "3  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n",
              "4  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...   \n",
              "\n",
              "                                       rna_motif_emb  \\\n",
              "0  [[0.02515462040901184, -0.1693081259727478, 1....   \n",
              "1  [[-0.005481339991092682, -0.008752591907978058...   \n",
              "2  [[-0.006157027557492256, -0.04564734920859337,...   \n",
              "3  [[0.0016195997595787048, 0.0697508156299591, 1...   \n",
              "4  [[0.02757852151989937, 0.006868686527013779, 3...   \n",
              "\n",
              "                                         rbp_esm_emb  \n",
              "0  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n",
              "1  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n",
              "2  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n",
              "3  [[0.102689214, -0.18220823, -0.05008613, 0.156...  \n",
              "4  [[0.102689214, -0.18220823, -0.05008613, 0.156...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2cc064a-e478-4922-923a-75b08ab4639f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gene_name</th>\n",
              "      <th>Gene_id</th>\n",
              "      <th>Motif</th>\n",
              "      <th>RBP_sequence</th>\n",
              "      <th>rna_motif_emb</th>\n",
              "      <th>rbp_esm_emb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>ENSG00000148584</td>\n",
              "      <td>UGAUCAGUAUA</td>\n",
              "      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n",
              "      <td>[[0.02515462040901184, -0.1693081259727478, 1....</td>\n",
              "      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>ENSG00000148584</td>\n",
              "      <td>AUAAUUA</td>\n",
              "      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n",
              "      <td>[[-0.005481339991092682, -0.008752591907978058...</td>\n",
              "      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>ENSG00000148584</td>\n",
              "      <td>UUAAUUA</td>\n",
              "      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n",
              "      <td>[[-0.006157027557492256, -0.04564734920859337,...</td>\n",
              "      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>ENSG00000148584</td>\n",
              "      <td>AUAAUUG</td>\n",
              "      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n",
              "      <td>[[0.0016195997595787048, 0.0697508156299591, 1...</td>\n",
              "      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>ENSG00000148584</td>\n",
              "      <td>UUAAUUG</td>\n",
              "      <td>MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...</td>\n",
              "      <td>[[0.02757852151989937, 0.006868686527013779, 3...</td>\n",
              "      <td>[[0.102689214, -0.18220823, -0.05008613, 0.156...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2cc064a-e478-4922-923a-75b08ab4639f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2cc064a-e478-4922-923a-75b08ab4639f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2cc064a-e478-4922-923a-75b08ab4639f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1efdea43-6d5e-4680-bf7c-15d312bcbb59\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1efdea43-6d5e-4680-bf7c-15d312bcbb59')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1efdea43-6d5e-4680-bf7c-15d312bcbb59 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 3252,\n  \"fields\": [\n    {\n      \"column\": \"Gene_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 158,\n        \"samples\": [\n          \"SRSF10\",\n          \"HNRNPAB\",\n          \"SRSF6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gene_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 159,\n        \"samples\": [\n          \"ENSG00000162231\",\n          \"ENSG00000152518\",\n          \"ENSG00000136450\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Motif\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2294,\n        \"samples\": [\n          \"ACCACGCA\",\n          \"AGGAGC\",\n          \"UGGGGAU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RBP_sequence\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 158,\n        \"samples\": [\n          \"MSRYLRPPNTSLFVRNVADDTRSEDLRREFGRYGPIVDVYVPLDFYTRRPRGFAYVQFEDVRDAEDALHNLDRKWICGRQIEIQFAQGDRKTPNQMKAKEGRNVYSSSRYDDYDRYRRSRSRSYERRRSRSRSFDYNYRRSYSPRNSRPTGRPRRSRSHSDNDRFKHRNRSFSRSKSNSRSRSKSQPKKEMKAKSRSRSASHTKTRGTSKTDSKTHYKSGSRYEKESRKKEPPRSKSQSRSQSRSRSKSRSRSWTSPKSSGH\",\n          \"MSEAGEEQPMETTGATENGHEAVPEASRGRGWTGAAAGAGGATAAPPSGNQNGAEGDQINASKNEEDAGKMFVGGLSWDTSKKDLKDYFTKFGEVVDCTIKMDPNTGRSRGFGFILFKDAASVEKVLDQKEHRLDGRVIDPKKAMAMKKDPVKKIFVGGLNPESPTEEKIREYFGEFGEIEAIELPMDPKLNKRRGFVFITFKEEEPVKKVLEKKFHTVSGSKCEIKVAQPKEVYQQQQYGSGGRGNRNRGNRGSGGGGGGGGQSQSWNQGYGNYWNQGYGYQQGYGPGYGGYDYSPYGYYGYGPGYDYSQGSTNYGKSQRRGGHQNNYKPY\",\n          \"MPRVYIGRLSYNVREKDIQRFFSGYGRLLEVDLKNGYGFVEFEDSRDADDAVYELNGKELCGERVIVEHARGPRRDRDGYSYGSRSGGGGYSSRRTSGRDKYGPPVRTEYRLIVENLSSRCSWQDLKDFMRQAGEVTYADAHKERTNEGVIEFRSYSDMKRALDKLDGTEINGRNIRLIEDKPRTSHRRSYSGSRSRSRSRRRSRSRSRRSSRSRSRSISKSRSRSRSRSKGRSRSRSKGRKSRSKSKSKPKSDRGSHSHSRSRSKDEYEKSRSRSRSRSPKENGKGDIKSKSRSRSQSRSNSPLPVPPSKARSVSPPPKRATSRSRSRSRSKSRSRSRSSSRD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rna_motif_emb\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rbp_esm_emb\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmseqs2 on dataset"
      ],
      "metadata": {
        "id": "KgWejYjbHKsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install condacolab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QICA3IdiHN4x",
        "outputId": "d5da760f-82f4-4a77-929c-37a5e6134874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge -c bioconda mmseqs2"
      ],
      "metadata": {
        "id": "0YeILLsWHmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mmseqs createdb sequences.fasta DB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "WgoxvV2AO9Ut",
        "outputId": "c691e02b-2a58-422f-c54b-898ebfa3becb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-10b17da785be>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-10b17da785be>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mmseqs createdb sequences.fasta DB\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEEhZ3HmN66q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "n--6yv9O_PD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import esm"
      ],
      "metadata": {
        "id": "77t0Jz_hBpNz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.d_k = config.d_model // config.num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        self.k_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        self.v_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        self.out = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs = x.size(0)\n",
        "        q = self.q_linear(x).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn, v)\n",
        "\n",
        "        context = context.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "        output = self.out(context)\n",
        "        return output"
      ],
      "metadata": {
        "id": "3AA3O328_Qm5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrastive_loss = nn.CosineEmbeddingLoss()\n",
        "ce_loss = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "pt4uxAZd_Z_L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## siamese network"
      ],
      "metadata": {
        "id": "mHGjlyEnqpIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6_ICsNy8gMCk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.fusion_dim,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=config.d_ff,\n",
        "            dropout=config.dropout_rate,\n",
        "            activation=F.gelu,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
        "        self.attention_pool = nn.MultiheadAttention(config.fusion_dim, config.num_heads, batch_first=True)\n",
        "\n",
        "        self.projection_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(config.fusion_dim, config.fusion_dim),\n",
        "                nn.LayerNorm(config.fusion_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(config.dropout_rate)\n",
        "            ) for _ in range(4)\n",
        "        ])\n",
        "\n",
        "        self.latent_projection = nn.Linear(config.fusion_dim, config.latent_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(config.latent_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        attn_output, attn_weights = self.attention_pool(x, x, x)\n",
        "        weighted_sum = x + torch.bmm(attn_weights, x)\n",
        "\n",
        "        for layer in self.projection_layers:\n",
        "            weighted_sum = layer(weighted_sum) + weighted_sum  # Residual connection\n",
        "\n",
        "        pooled = weighted_sum.mean(dim=1)\n",
        "        latent = self.latent_projection(pooled)\n",
        "        latent = self.final_layer_norm(self.dropout(F.gelu(latent)))\n",
        "        return latent\n",
        "\n",
        "class SiameseDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=config.latent_dim,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=config.d_ff,\n",
        "            dropout=config.dropout_rate,\n",
        "            activation=F.gelu,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=config.num_decoder_layers)\n",
        "\n",
        "        # Custom output projection\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(config.latent_dim, config.latent_dim),\n",
        "            nn.LayerNorm(config.latent_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.latent_dim, config.latent_dim),\n",
        "            nn.LayerNorm(config.latent_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.latent_dim, config.vocab_size)\n",
        "        )\n",
        "\n",
        "        # Projection to 1280 dimensions for ESM-2 compatibility\n",
        "        self.esm_projection = nn.Sequential(\n",
        "            nn.Linear(config.latent_dim, config.latent_dim),\n",
        "            nn.LayerNorm(config.latent_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.latent_dim, config.latent_dim),\n",
        "            nn.LayerNorm(config.latent_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.latent_dim, 1280)\n",
        "        )\n",
        "\n",
        "\n",
        "        # ESM-2 language model head\n",
        "        esm_model, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "        self.lm_head = esm_model.lm_head\n",
        "\n",
        "    def forward(self, latent, use_lm_head=True):\n",
        "        batch_size = latent.size(0)\n",
        "        seq_len = self.config.max_protein_length\n",
        "\n",
        "        decoded = self.decoder(\n",
        "            tgt=torch.zeros(batch_size, seq_len, self.config.latent_dim).to(latent.device),\n",
        "            memory=latent.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        )\n",
        "\n",
        "        if use_lm_head:\n",
        "            decoded_projected = self.esm_projection(decoded)\n",
        "            logits = self.lm_head(decoded_projected)\n",
        "        else:\n",
        "            logits = self.output_projection(decoded)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.rna_projection = self.create_projection_layers(config.rna_embed_dim, config.fusion_dim)\n",
        "        self.protein_projection = self.create_projection_layers(config.protein_embed_dim, config.fusion_dim)\n",
        "        self.siamese_network = SiameseNetwork(config)\n",
        "        self.siamese_decoder = SiameseDecoder(config)\n",
        "        esm_model, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "        self.lm_head = esm_model.lm_head\n",
        "\n",
        "    def create_projection_layers(self, input_dim, output_dim):\n",
        "        layers = []\n",
        "        current_dim = input_dim\n",
        "        for _ in range(3):  # 3 intermediate layers\n",
        "            layers.extend([\n",
        "                nn.Linear(current_dim, output_dim),\n",
        "                nn.LayerNorm(output_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(self.config.dropout_rate)\n",
        "            ])\n",
        "            current_dim = output_dim\n",
        "        layers.append(nn.Linear(current_dim, output_dim))  # Final projection\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, rna_emb, protein_emb=None, use_lm_head=True):\n",
        "        rna_proj = self.rna_projection(rna_emb)\n",
        "        rna_latent = self.siamese_network(rna_proj)\n",
        "        print('rna siamese done...')\n",
        "        # print('rna_proj:',rna_proj.shape)\n",
        "        # print('rna_latent:',rna_latent.shape)\n",
        "        # print()\n",
        "\n",
        "        if protein_emb is not None:\n",
        "            protein_proj = self.protein_projection(protein_emb)\n",
        "            protein_latent = self.siamese_network(protein_proj)\n",
        "        else:\n",
        "            protein_latent = None\n",
        "\n",
        "        print('protein siamese done...')\n",
        "        # print('protein_proj:',protein_proj.shape)\n",
        "        # print('protein_latent:',protein_latent.shape)\n",
        "        # print()\n",
        "\n",
        "        # print('decoder starting inputs...')\n",
        "        # print('rna_latent:',rna_latent.shape)\n",
        "        # print('use_lm_head:',use_lm_head)\n",
        "\n",
        "        generated_sequence = self.siamese_decoder(rna_latent, use_lm_head)\n",
        "        print('decoder done...')\n",
        "        print()\n",
        "\n",
        "        return rna_latent, protein_latent, generated_sequence\n",
        "\n",
        "def generate_peptides(model, token_representations, num_samples, sample_variances):\n",
        "    generated_peptides = []\n",
        "    aa_toks = list(\"ARNDCEQGHILKMFPSTWYV\")\n",
        "    aa_idxs = [alphabet.get_idx(aa) for aa in aa_toks]\n",
        "\n",
        "    for i in sample_variances:\n",
        "        for j in range(num_samples):\n",
        "            gen_pep = token_representations + torch.randn(token_representations.shape) * i * token_representations.var()\n",
        "            aa_logits = model.lm_head(gen_pep.cuda())[:, :, aa_idxs]\n",
        "            predictions = torch.argmax(aa_logits, dim=2).tolist()[0]\n",
        "            generated_pep_seq = \"\".join([aa_toks[i] for i in predictions])\n",
        "            generated_peptides.append(generated_pep_seq[1:-1])\n",
        "\n",
        "    return generated_peptides"
      ],
      "metadata": {
        "id": "miogZlePqoAg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10RSPRkw29E6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training (dummy dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "FVScx-_8Lyfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import esm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Config:\n",
        "    vocab_size = 33  # ESM-2 vocabulary size\n",
        "    pad_token = 0\n",
        "    start_token = 1\n",
        "    end_token = 2\n",
        "    rna_embed_dim = 120\n",
        "    protein_embed_dim = 1280\n",
        "    fusion_dim = 512\n",
        "    latent_dim = 256\n",
        "    d_model = 512\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    num_decoder_layers = 6\n",
        "    d_ff = 2048\n",
        "    dropout_rate = 0.1\n",
        "    max_rna_length = 100\n",
        "    max_protein_length = 200\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "    learning_rate = 3e-4\n",
        "\n",
        "config = Config()\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000):\n",
        "        self.num_samples = num_samples\n",
        "        self.data = self.generate_dummy_data()\n",
        "\n",
        "    def generate_dummy_data(self):\n",
        "        data = []\n",
        "        for _ in range(self.num_samples):\n",
        "            rna_length = random.randint(20, config.max_rna_length)\n",
        "            protein_length = random.randint(20, config.max_protein_length)\n",
        "            rna_emb = torch.randn(rna_length, config.rna_embed_dim)\n",
        "            protein_emb = torch.randn(protein_length, config.protein_embed_dim)\n",
        "            protein_seq = torch.randint(0, config.vocab_size, (protein_length,))\n",
        "            label = random.randint(0, 1)\n",
        "            data.append((rna_emb, protein_emb, protein_seq, label))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    rna_embs, protein_embs, protein_seqs, labels = zip(*batch)\n",
        "\n",
        "    max_rna_len = max(emb.size(0) for emb in rna_embs)\n",
        "    max_protein_len = max(emb.size(0) for emb in protein_embs)\n",
        "    print(max_protein_len)\n",
        "\n",
        "    padded_rna_emb = torch.zeros(len(batch), max_rna_len, config.rna_embed_dim)\n",
        "    padded_protein_emb = torch.zeros(len(batch), max_protein_len, config.protein_embed_dim)\n",
        "    padded_protein_seq = torch.full((len(batch), max_protein_len), config.vocab_size - 1)\n",
        "\n",
        "    for i, (rna_emb, protein_emb, protein_seq) in enumerate(zip(rna_embs, protein_embs, protein_seqs)):\n",
        "        padded_rna_emb[i, :rna_emb.size(0)] = rna_emb\n",
        "        padded_protein_emb[i, :protein_emb.size(0)] = protein_emb\n",
        "        padded_protein_seq[i, :protein_seq.size(0)] = protein_seq\n",
        "\n",
        "    return padded_rna_emb, padded_protein_emb, padded_protein_seq, torch.tensor(labels)\n",
        "\n",
        "train_dataset = DummyDataset()\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = CombinedModel(config)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "contrastive_loss = nn.CosineEmbeddingLoss()\n",
        "ce_loss = nn.CrossEntropyLoss(ignore_index=config.vocab_size - 1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print('model prep done...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mCs8gNApOLE",
        "outputId": "c01a7961-de86-49f0-ffec-a67e07087b7c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model prep done...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(config.num_epochs):\n",
        "    model.train()\n",
        "    total_loss=0\n",
        "    for rna_embs,protein_embs,target_seqs,labels in train_loader:\n",
        "        rna_embs,protein_embs,target_seqs,labels=rna_embs.to(device),protein_embs.to(device),target_seqs.to(device),labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rna_latent,protein_latent,generated_sequence=model(rna_embs,protein_embs,use_lm_head=True)\n",
        "        print(\"generated sequence\",generated_sequence.shape)\n",
        "        print(\"target seqs\",target_seqs.shape)\n",
        "\n",
        "        loss_contrastive=contrastive_loss(rna_latent,protein_latent,(labels*2-1).float())\n",
        "        loss_generation=ce_loss(generated_sequence.view(-1,config.vocab_size),target_seqs.view(-1))\n",
        "\n",
        "        loss=loss_contrastive+loss_generation\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss+=loss.item()\n",
        "\n",
        "    avg_loss=total_loss/len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{config.num_epochs}, Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "cXBED5Wp0MgK",
        "outputId": "60299097-bf1c-4cdf-c61d-1b2f1d6e24b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "184\n",
            "rna siamese done...\n",
            "protein siamese done...\n",
            "decoder done...\n",
            "\n",
            "generated sequence torch.Size([32, 200, 33])\n",
            "target seqs torch.Size([32, 184])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (6400) to match target batch_size (5888).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-deaa5d98f450>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_contrastive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrna_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotein_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_contrastive\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1189\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (6400) to match target batch_size (5888)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WovNaFyL_Y8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}